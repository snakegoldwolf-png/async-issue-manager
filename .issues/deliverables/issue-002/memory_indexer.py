#!/usr/bin/env python3
"""
è®°å¿†ç´¢å¼•ç³»ç»Ÿ - æä¾›å¿«é€Ÿæ£€ç´¢èƒ½åŠ›
åŸºäº Hunter çš„åˆ†æï¼Œå®ç°æ°¸ç»­ Agent çš„ç¬¬ä¸‰å±‚è®°å¿†æ¶æ„
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime
import hashlib


class MemoryIndexer:
    """è®°å¿†ç´¢å¼•å™¨ - æ„å»ºå’Œç»´æŠ¤è®°å¿†ç´¢å¼•"""
    
    def __init__(self, workspace_dir: Path):
        self.workspace_dir = workspace_dir
        self.memory_dir = workspace_dir / "memory"
        self.index_file = self.memory_dir / "index.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.memory_dir.mkdir(parents=True, exist_ok=True)
    
    def build_index(self) -> Dict:
        """æ„å»ºå®Œæ•´ç´¢å¼•"""
        index = {
            "keywords": {},
            "tags": {},
            "priorities": {},
            "files": {},
            "last_updated": datetime.now().isoformat(),
            "version": "1.0.0"
        }
        
        # ç´¢å¼• MEMORY.md
        memory_file = self.workspace_dir / "MEMORY.md"
        if memory_file.exists():
            self._index_file(memory_file, index)
        
        # ç´¢å¼•æ¯æ—¥æ—¥å¿—
        for daily_file in self.memory_dir.glob("*.md"):
            if daily_file.name != "index.json":
                self._index_file(daily_file, index)
        
        return index
    
    def _index_file(self, file_path: Path, index: Dict):
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return
        
        # æ–‡ä»¶ç›¸å¯¹è·¯å¾„
        rel_path = str(file_path.relative_to(self.workspace_dir))
        
        # æ–‡ä»¶å…ƒæ•°æ®
        file_hash = hashlib.md5(content.encode()).hexdigest()
        index["files"][rel_path] = {
            "size": len(content),
            "hash": file_hash,
            "last_modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
        }
        
        # æå–æ ‡é¢˜å’Œé”šç‚¹
        sections = self._extract_sections(content, rel_path)
        
        # æå–å…³é”®è¯
        keywords = self._extract_keywords(content)
        for keyword in keywords:
            if keyword not in index["keywords"]:
                index["keywords"][keyword] = []
            index["keywords"][keyword].append(rel_path)
        
        # æå–æ ‡ç­¾
        tags = self._extract_tags(content)
        for tag in tags:
            if tag not in index["tags"]:
                index["tags"][tag] = []
            index["tags"][tag].append(rel_path)
        
        # æå–ä¼˜å…ˆçº§
        priorities = self._extract_priorities(content, rel_path)
        for priority, locations in priorities.items():
            if priority not in index["priorities"]:
                index["priorities"][priority] = []
            index["priorities"][priority].extend(locations)
    
    def _extract_sections(self, content: str, file_path: str) -> List[Dict]:
        """æå–ç« èŠ‚æ ‡é¢˜å’Œé”šç‚¹"""
        sections = []
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            # åŒ¹é… Markdown æ ‡é¢˜
            match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if match:
                level = len(match.group(1))
                title = match.group(2).strip()
                
                # ç”Ÿæˆé”šç‚¹
                anchor = self._generate_anchor(title)
                
                sections.append({
                    "level": level,
                    "title": title,
                    "anchor": anchor,
                    "line": i + 1,
                    "location": f"{file_path}#{anchor}"
                })
        
        return sections
    
    def _generate_anchor(self, title: str) -> str:
        """ç”Ÿæˆ Markdown é”šç‚¹"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œè½¬å°å†™ï¼Œç©ºæ ¼è½¬è¿å­—ç¬¦
        anchor = re.sub(r'[^\w\s-]', '', title)
        anchor = re.sub(r'\s+', '-', anchor)
        return anchor.lower()
    
    def _extract_keywords(self, content: str) -> Set[str]:
        """æå–å…³é”®è¯"""
        keywords = set()
        
        # å¸¸è§æŠ€æœ¯å…³é”®è¯æ¨¡å¼
        patterns = [
            r'\b(bug|feature|enhancement|hotfix|critical)\b',
            r'\b(PR|Issue|commit|merge|review)\b',
            r'\b(bounty|Algora|GitHub|Nuclei)\b',
            r'\b(Python|JavaScript|TypeScript|Rust|Go)\b',
            r'\b(API|CLI|SDK|UI|UX)\b',
            r'\b(æµ‹è¯•|éƒ¨ç½²|å‘å¸ƒ|å›æ»š)\b',
            r'\b(ä¼˜åŒ–|é‡æ„|æ¸…ç†|å½’æ¡£)\b',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.update([m.lower() for m in matches])
        
        return keywords
    
    def _extract_tags(self, content: str) -> Set[str]:
        """æå–æ ‡ç­¾ï¼ˆ#tag æ ¼å¼ï¼‰"""
        tags = set()
        
        # åŒ¹é… #tag æ ¼å¼
        matches = re.findall(r'#(\w+)', content)
        tags.update(matches)
        
        return tags
    
    def _extract_priorities(self, content: str, file_path: str) -> Dict[str, List[str]]:
        """æå–ä¼˜å…ˆçº§æ ‡è®°"""
        priorities = {}
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            # åŒ¹é… [P0], [P1], [P2], [P3]
            match = re.search(r'\[P([0-3])\]', line)
            if match:
                priority = f"P{match.group(1)}"
                
                # å°è¯•æå–æ ‡é¢˜
                title_match = re.search(r'\[P[0-3]\]\s+(.+)', line)
                title = title_match.group(1).strip() if title_match else f"Line {i+1}"
                
                location = f"{file_path}#L{i+1}"
                
                if priority not in priorities:
                    priorities[priority] = []
                priorities[priority].append({
                    "title": title,
                    "location": location
                })
        
        return priorities
    
    def save_index(self, index: Dict):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç´¢å¼•å·²ä¿å­˜: {self.index_file}")
    
    def load_index(self) -> Dict:
        """åŠ è½½ç´¢å¼•"""
        if not self.index_file.exists():
            return None
        
        with open(self.index_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def search(self, query: str, index: Dict = None) -> List[Dict]:
        """æœç´¢è®°å¿†"""
        if index is None:
            index = self.load_index()
            if index is None:
                return []
        
        results = []
        query_lower = query.lower()
        
        # æœç´¢å…³é”®è¯
        if query_lower in index["keywords"]:
            for file_path in index["keywords"][query_lower]:
                results.append({
                    "type": "keyword",
                    "query": query,
                    "file": file_path,
                    "relevance": 1.0
                })
        
        # æœç´¢æ ‡ç­¾
        if query in index["tags"]:
            for file_path in index["tags"][query]:
                results.append({
                    "type": "tag",
                    "query": query,
                    "file": file_path,
                    "relevance": 0.9
                })
        
        # æœç´¢ä¼˜å…ˆçº§
        if query.upper() in index["priorities"]:
            for item in index["priorities"][query.upper()]:
                results.append({
                    "type": "priority",
                    "query": query,
                    "title": item["title"],
                    "location": item["location"],
                    "relevance": 0.8
                })
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        results.sort(key=lambda x: x["relevance"], reverse=True)
        
        return results
    
    def get_stats(self, index: Dict = None) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if index is None:
            index = self.load_index()
            if index is None:
                return {}
        
        return {
            "total_files": len(index["files"]),
            "total_keywords": len(index["keywords"]),
            "total_tags": len(index["tags"]),
            "priorities": {
                "P0": len(index["priorities"].get("P0", [])),
                "P1": len(index["priorities"].get("P1", [])),
                "P2": len(index["priorities"].get("P2", [])),
                "P3": len(index["priorities"].get("P3", []))
            },
            "last_updated": index["last_updated"]
        }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è®°å¿†ç´¢å¼•ç³»ç»Ÿ")
    subparsers = parser.add_subparsers(dest="command", help="å‘½ä»¤")
    
    # build å‘½ä»¤
    build_parser = subparsers.add_parser("build", help="æ„å»ºç´¢å¼•")
    build_parser.add_argument("--workspace", default=os.path.expanduser("~/.openclaw/workspace"), help="å·¥ä½œåŒºç›®å½•")
    
    # search å‘½ä»¤
    search_parser = subparsers.add_parser("search", help="æœç´¢è®°å¿†")
    search_parser.add_argument("query", help="æœç´¢å…³é”®è¯")
    search_parser.add_argument("--workspace", default=os.path.expanduser("~/.openclaw/workspace"), help="å·¥ä½œåŒºç›®å½•")
    
    # stats å‘½ä»¤
    stats_parser = subparsers.add_parser("stats", help="æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")
    stats_parser.add_argument("--workspace", default=os.path.expanduser("~/.openclaw/workspace"), help="å·¥ä½œåŒºç›®å½•")
    
    args = parser.parse_args()
    
    workspace_dir = Path(args.workspace)
    indexer = MemoryIndexer(workspace_dir)
    
    if args.command == "build":
        print("ğŸ”¨ æ„å»ºè®°å¿†ç´¢å¼•...")
        index = indexer.build_index()
        indexer.save_index(index)
        
        stats = indexer.get_stats(index)
        print(f"\nğŸ“Š ç´¢å¼•ç»Ÿè®¡:")
        print(f"  æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"  å…³é”®è¯: {stats['total_keywords']}")
        print(f"  æ ‡ç­¾: {stats['total_tags']}")
        print(f"  ä¼˜å…ˆçº§: P0={stats['priorities']['P0']}, P1={stats['priorities']['P1']}, P2={stats['priorities']['P2']}, P3={stats['priorities']['P3']}")
    
    elif args.command == "search":
        results = indexer.search(args.query)
        
        if results:
            print(f"ğŸ” æœç´¢ç»“æœ ({len(results)}):\n")
            for i, result in enumerate(results, 1):
                print(f"{i}. [{result['type']}] {result.get('file', result.get('location', 'N/A'))}")
                if 'title' in result:
                    print(f"   æ ‡é¢˜: {result['title']}")
                print(f"   ç›¸å…³åº¦: {result['relevance']:.1%}\n")
        else:
            print(f"âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ: {args.query}")
    
    elif args.command == "stats":
        stats = indexer.get_stats()
        if stats:
            print("ğŸ“Š ç´¢å¼•ç»Ÿè®¡:")
            print(f"  æ–‡ä»¶æ•°: {stats['total_files']}")
            print(f"  å…³é”®è¯: {stats['total_keywords']}")
            print(f"  æ ‡ç­¾: {stats['total_tags']}")
            print(f"  ä¼˜å…ˆçº§: P0={stats['priorities']['P0']}, P1={stats['priorities']['P1']}, P2={stats['priorities']['P2']}, P3={stats['priorities']['P3']}")
            print(f"  æœ€åæ›´æ–°: {stats['last_updated']}")
        else:
            print("âŒ ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build å‘½ä»¤")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
